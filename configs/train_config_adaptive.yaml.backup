# Adaptive Training Configuration for NT8 RL Trading Strategy
# This config is optimized for models that aren't trading and includes
# adaptive parameter adjustment during training

# Model Configuration
model:
  algorithm: "PPO"
  learning_rate: 0.0001
  batch_size: 128
  n_steps: 4096
  n_epochs: 30
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  value_loss_coef: 0.5
  entropy_coef: 0.05  # INCREASED from 0.025 to encourage more exploration
  max_grad_norm: 0.5
  
  # Network architecture
  hidden_dims: [256, 256, 128]

# Environment Configuration
environment:
  instrument: "ES"
  timeframes: [1, 5, 15]
  state_features: 900
  action_space: "continuous"
  action_range: [-1.0, 1.0]  # Consider widening to [-1.5, 1.5] if still no trades
  action_threshold: 0.02  # Reduced from 0.05 to 0.02 (2%) to allow more trades during training - can increase back to 0.05 once trade count stabilizes
  max_episode_steps: 10000
  lookback_bars: 20
  
  trading_hours:
    enabled: true
    source_timezone: "America/New_York"
    exclude_weekends: true
    use_nt8_calendar: true
    sessions:
      - name: "Tokyo"
        timezone: "Asia/Tokyo"
        start: "09:00"
        end: "15:00"
      - name: "London"
        timezone: "Europe/London"
        start: "08:00"
        end: "16:30"
      - name: "New York"
        timezone: "America/New_York"
        start: "09:30"
        end: "16:00"
  
  # Reward function parameters (optimized for profitability)
  reward:
    pnl_weight: 1.0
    transaction_cost: 0.0003  # Increased to 0.03% for realistic costs (commission + slippage)
    risk_penalty: 0.05  # REDUCED from 0.1 to allow more risk-taking
    drawdown_penalty: 0.05  # REDUCED from 0.1
    # Profitability-focused reward configuration
    exploration_bonus_enabled: true  # Can be disabled
    exploration_bonus_scale: 0.00001  # Reduced from 0.0001 (10x reduction) - only if few trades
    loss_mitigation: 0.05  # Reduced from 0.3 to 0.05 (5% mitigation)
    overtrading_penalty_enabled: true
    optimal_trades_per_episode: 50  # Target trades per episode
    profit_factor_required: 1.0  # Minimum profit factor to reward
    inaction_penalty: 0.0001  # Base penalty for not trading (will adapt during training)
    max_consecutive_losses: 3  # Stop trading after N consecutive losses (prevents revenge trading)
    resume_confluence_required: 3  # Require confluence >= N to resume trading after pause (for live trading)
    # CRITICAL FIX: Stop loss and risk/reward ratio to fix profitability
    stop_loss_pct: 0.02  # Stop loss at 2% of entry price (caps maximum loss per trade)
    min_risk_reward_ratio: 1.5  # Minimum risk/reward ratio (1.5:1) - reject trades with poor R:R
    # Simplified quality filters for training (applied in TradingEnvironment, mirrors DecisionGate)
    quality_filters:
      enabled: true  # Enable quality filters during training
      min_action_confidence: 0.15  # Increased from 0.1 to 0.15 to reduce trade count and improve quality
      min_quality_score: 0.4  # Increased from 0.3 to 0.4 to reduce trade count and improve quality
      require_positive_expected_value: false  # Disabled for training - EV calculation needs more data

# Training Configuration
training:
  total_timesteps: 1000000  # Can continue training
  save_freq: 10000
  use_decision_gate: true  # Enable DecisionGate integration in training loop (ensures consistency with live trading)
  eval_freq: 10000  # Evaluate every 10k steps for adaptive adjustments
  device: "cuda"
  num_envs: 1
  transfer_learning: true
  transfer_checkpoint: "models/best_model.pt"
  transfer_strategy: "copy_and_extend"
  
  # Adaptive training settings
  adaptive_training:
    enabled: true  # Enable adaptive parameter adjustment
    eval_frequency: 10000  # Evaluate every N timesteps
    eval_episodes: 3  # Episodes per evaluation
    min_trades_per_episode: 0.5  # Minimum trades per episode
    min_win_rate: 0.35
    target_sharpe: 0.5
    auto_save_on_improvement: true
    improvement_threshold: 0.05  # 5% improvement triggers save
  
  # Data
  train_split: 0.8
  validation_split: 0.1
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 50000  # More patience for adaptive training
    min_delta: 0.005

# Risk Management
risk_management:
  max_position_size: 1.0
  max_drawdown: 0.20
  stop_loss_atr_multiplier: 2.0
  initial_capital: 100000.0
  commission: 2.0
  max_position_fraction_of_balance: 0.02
  position_value_per_unit: 5000.0
  break_even:
    enabled: true
    activation_pct: 0.006  # Move to break-even after 0.6% profit (2x commission at 0.03%)
    trail_pct: 0.0015  # Trailing stop at 0.15% (1:2 risk/reward ratio)
    scale_out_fraction: 0.5  # Scale out 50% when profitable
    scale_out_min_confluence: 2
    free_trade_fraction: 0.5  # Protect 50% of position as free trade

# Drift Detection
drift_detection:
  enabled: true
  baseline_metrics:
    win_rate: 0.55
    sharpe_ratio: 1.0
    profit_factor: 1.5
    max_drawdown: 0.15
  thresholds:
    win_rate_drop: 0.10
    sharpe_drop: 0.30
    profit_factor_drop: 0.30
    max_drawdown_increase: 0.05
    consecutive_losses: 5
  window_size: 50
  min_trades: 20

# Reasoning Engine
reasoning:
  enabled: true
  provider: "ollama"
  model: "deepseek-r1:8b"
  api_key: null
  base_url: null
  keep_alive: "10m"
  pre_trade_validation: true
  confidence_threshold: 0.7
  timeout: 2.0
  use_kong: false
  kong_base_url: "http://localhost:8300"
  kong_api_key: null

# Agentic Swarm Configuration
agentic_swarm:
  enabled: true
  provider: "ollama"
  keep_alive: "10m"
  max_handoffs: 10
  max_iterations: 15
  execution_timeout: 20.0
  node_timeout: 5.0
  cache_ttl: 300
  
  contrarian:
    enabled: true
    greed_threshold_percentile: 90
    fear_threshold_percentile: 10
  
  market_research:
    instruments: ["ES", "NQ", "RTY", "YM"]
    correlation_window: 20
    divergence_threshold: 0.1
  
  sentiment:
    sources: ["newsapi"]
    newsapi_key: null
    sentiment_window: 3600
  
  elliott_wave:
    enabled: true
    lookback_bars: 400
    swing_threshold: 0.003
    min_confidence: 0.55
    position_multiplier: 0.6
    max_position_size: 0.8
    min_bars: 120
  
  analyst:
    deep_reasoning: true
    conflict_detection: true
  
  recommendation:
    risk_integration: true
    position_sizing: true

# Decision Gate Configuration
decision_gate:
  rl_weight: 0.6
  swarm_weight: 0.4
  min_combined_confidence: 0.7
  conflict_reduction_factor: 0.5
  min_confluence_required: 2  # Require minimum confluence >= 2 for all trades (configurable)
  swarm_enabled: true
  swarm_timeout: 20.0
  fallback_to_rl_only: true
  # Quality scorer configuration
  quality_scorer:
    enabled: true  # Enable quality scoring
    min_quality_score: 0.6  # Minimum quality score to trade (0-1)
    min_risk_reward_ratio: 1.5  # Minimum risk/reward ratio (1:1.5)
    min_profit_margin: 1.5  # Expected profit must be >= commission * 1.5
    min_volatility: 0.01  # Minimum volatility to trade
    min_volume_ratio: 1.2  # Volume must be >= avg * 1.2
    # Quality score weights (must sum to 1.0)
    confidence_weight: 0.30
    confluence_weight: 0.20
    expected_profit_weight: 0.20
    risk_reward_weight: 0.15
    market_conditions_weight: 0.15
  position_sizing:
    enabled: true
    scale_multipliers:
      "1": 1.0
      "2": 1.15
      "3": 1.3
    max_scale: 1.3
    min_scale: 0.3
    hold_scale: 0.3
    rl_only_scale: 0.5
    swarm_only_scale: 0.65
    disagreement_scale: 0.35
    contrarian_threshold: 0.6
    elliott_threshold: 0.6

# Manual Approval Configuration
manual_approval:
  enabled: false
  auto_approve_timeout: 30.0
  require_approval_for:
    - "high_risk_trades"
    - "conflicting_signals"
    - "large_position_sizes"

# Logging
logging:
  log_dir: "logs"
  tensorboard: true
  verbose: 1

# Live Trading Configuration
live_trading:
  enabled: false
  paper_trading: true

# Bridge Configuration
bridge:
  host: "localhost"
  port: 8888

# Continuous Learning Configuration
continuous_learning:
  retrain_frequency: 1000
  min_experiences: 500
  evaluation_episodes: 10
  min_annotated_for_finetune: 100
  experience_buffer_size: 10000
  experience_storage: "data/experience_buffer"

