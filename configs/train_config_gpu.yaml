# GPU-Optimized Training Configuration for NT8 RL Trading Strategy
# Optimized for low-end to mid-range GPUs (e.g., RTX 2070, RTX 3060)
# This config provides ~2-3x faster training compared to default

# Model Configuration
model:
  algorithm: "PPO"                    # PPO, SAC, DQN
  learning_rate: 0.0003               # Adam learning rate
  batch_size: 128                     # Increased batch size for GPU efficiency (was 64)
  n_steps: 2048                       # Steps per update
  n_epochs: 4                         # Reduced epochs per update (was 10) - less overfitting, faster
  gamma: 0.99                         # Discount factor
  gae_lambda: 0.95                    # GAE lambda
  clip_range: 0.2                     # PPO clip range
  value_loss_coef: 0.5                # Value function loss coefficient
  entropy_coef: 0.01                  # Entropy bonus coefficient
  max_grad_norm: 0.5                  # Gradient clipping
  
  # Network architecture (smaller for speed)
  hidden_dims: [128, 128, 64]         # Reduced from [256, 256, 128] - ~4x fewer parameters

# Environment Configuration
environment:
  instrument: "ES"                     # ES or MES
  timeframes: [1, 5, 15]             # Timeframes in minutes
  state_features: 900                 # Must match environment state_dim (15 features * 3 timeframes * 20 lookback = 900)
  action_space: "continuous"          # continuous or discrete
  action_range: [-1.0, 1.0]          # Position size range
  trading_hours:
    enabled: true
    source_timezone: "America/New_York"   # NT8 export timezone (CME / US Eastern)
    exclude_weekends: true
    use_nt8_calendar: true
    sessions:
      - name: "Tokyo"
        timezone: "Asia/Tokyo"
        start: "09:00"
        end: "15:00"
      - name: "London"
        timezone: "Europe/London"
        start: "08:00"
        end: "16:30"
      - name: "New York"
        timezone: "America/New_York"
        start: "09:30"
        end: "16:00"
  
  # Reward function parameters
  reward:
    pnl_weight: 1.0                   # PnL contribution to reward
    transaction_cost: 0.0001          # Transaction cost per trade
    risk_penalty: 0.1                  # Reduced to limit idle bleed
    drawdown_penalty: 0.3             # Drawdown penalty

# Training Configuration
training:
  total_timesteps: 500000             # Reduced for faster testing (can increase later)
  save_freq: 10000                   # Save checkpoint frequency
  eval_freq: 10000                    # Less frequent evaluation (was 5000)
  device: "cuda"                     # cuda or cpu
  num_envs: 1                        # Number of parallel environments
  
  # Performance optimizations
  use_mixed_precision: false         # DISABLED - Can cause NaN issues. Enable only if you need extra speed.
  compile_model: false                # PyTorch 2.0 compile (can add ~10-20% but requires PyTorch 2.0+)
  
  # Data
  train_split: 0.8                   # Train/test split
  validation_split: 0.1              # Validation split
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 50000                   # Steps to wait without improvement
    min_delta: 0.01                   # Minimum improvement

# Risk Management
risk_management:
  max_position_size: 1.0              # Maximum position size (normalized)
  max_drawdown: 0.20                  # Maximum drawdown (20%)
  stop_loss_atr_multiplier: 2.0       # Stop loss as multiple of ATR
  initial_capital: 100000.0          # Starting capital (for paper trading)
  commission: 2.0                     # Commission per contract per side
  max_position_fraction_of_balance: 0.02  # Max risk per trade relative to equity (2%)
  position_value_per_unit: 5000.0     # Notional value represented by position size 1.0
  break_even:
    enabled: true
    activation_pct: 0.0015
    trail_pct: 0.001
    scale_out_fraction: 0.3
    scale_out_min_confluence: 2
    free_trade_fraction: 0.3

# Drift Detection for Live Trading
drift_detection:
  enabled: true                       # Enable drift monitoring
  baseline_metrics:
    win_rate: 0.55                    # Baseline win rate
    sharpe_ratio: 1.0                 # Baseline Sharpe ratio
    profit_factor: 1.5                # Baseline profit factor
    max_drawdown: 0.15                # Baseline max drawdown
  thresholds:
    win_rate_drop: 0.10               # Alert if win rate drops by 10%
    sharpe_drop: 0.30                 # Alert if Sharpe drops by 0.3
    profit_factor_drop: 0.30          # Alert if PF drops by 30%
    max_drawdown_increase: 0.05       # Alert if DD increases by 5%
    consecutive_losses: 5             # Alert after 5 losses in a row
  window_size: 50                     # Number of trades to analyze
  min_trades: 20                      # Min trades before drift detection

# Reasoning Engine (DeepSeek-R1)
reasoning:
  enabled: false                      # DISABLED during training for speed (can enable after)
  model: "deepseek-r1:8b"            # Ollama model name
  pre_trade_validation: true         # Validate before trading
  confidence_threshold: 0.7           # Minimum confidence to trade
  timeout: 2.0                        # Reasoning timeout (seconds)

# Agentic Swarm Configuration (used in live trading/backtests)
agentic_swarm:
  enabled: true                       # Enable agentic swarm system
  execution_timeout: 20.0             # Total execution timeout (seconds)
  contrarian:
    enabled: true                     # Enable Warren Buffett-style contrarian agent
    greed_threshold_percentile: 90    # Top 10% sentiment/volatility = greed
    fear_threshold_percentile: 10     # Bottom 10% sentiment/volatility = fear
  elliott_wave:
    enabled: false
    min_confidence: 0.45
    swing_threshold: 0.0030
    lookback_bars: 450
    position_multiplier: 0.75
    max_position_size: 0.85

# Logging
logging:
  log_dir: "logs"                     # Log directory
  tensorboard: true                   # Enable TensorBoard
  verbose: 1                          # Verbosity level (0, 1, 2)

# Live Trading Configuration
live_trading:
  enabled: false                      # Enable live trading (USE WITH CAUTION)
  paper_trading: true                 # Paper trading mode (default)

# Bridge Configuration
bridge:
  host: "localhost"                  # NT8 bridge server host
  port: 8888                         # NT8 bridge server port

# Decision Gate Configuration
decision_gate:
  rl_weight: 0.55
  swarm_weight: 0.45
  reasoning_weight: 0.4               # Weight for reasoning confidence
  min_combined_confidence: 0.5        # Minimum confidence to execute trade
  conflict_reduction_factor: 0.7      # Reduce position size when RL/reasoning disagree
  swarm_enabled: true
  position_sizing:
    enabled: true
    scale_multipliers:
      "1": 1.0
      "2": 1.1
      "3": 1.3
    max_scale: 1.25
    min_scale: 0.3
    hold_scale: 0.3
    rl_only_scale: 0.5
    swarm_only_scale: 0.65
    disagreement_scale: 0.35
    contrarian_threshold: 0.6
    elliott_threshold: 0.6

# Continuous Learning Configuration
continuous_learning:
  retrain_frequency: 1000            # Retrain every N new experiences
  min_experiences: 500                # Minimum experiences before retraining
  evaluation_episodes: 10             # Episodes for model evaluation
  min_annotated_for_finetune: 100     # Minimum annotated experiences for DeepSeek fine-tuning
  experience_buffer_size: 10000       # Maximum experiences to store
  experience_storage: "data/experience_buffer"

