# Training Configuration for NT8 RL Trading Strategy

# Model Configuration
model:
  algorithm: "PPO"                    # PPO, SAC, DQN
  learning_rate: 0.0003               # Adam learning rate
  batch_size: 128                     # Batch size (increased for Turbo mode - with 50x multiplier = 6400 batch size)
  n_steps: 4096                       # Steps per update (INCREASED from 2048 - larger buffer = more data per update = more GPU work)
  n_epochs: 30                        # Epochs per update (increased from default 10 - more training per update = more GPU work)
  gamma: 0.99                         # Discount factor
  gae_lambda: 0.95                    # GAE lambda
  clip_range: 0.2                     # PPO clip range
  value_loss_coef: 0.5                # Value function loss coefficient
  entropy_coef: 0.01                  # Entropy bonus coefficient
  max_grad_norm: 0.5                  # Gradient clipping
  
  # Network architecture (for future training - larger model = more VRAM usage)
  hidden_dims: [256, 256, 128]       # Larger model for better GPU utilization (was [128, 128, 64] in checkpoint)

# Environment Configuration
environment:
  instrument: "ES"                     # ES or MES
  timeframes: [1, 5, 15]             # Timeframes in minutes
  state_features: 200                 # State dimension (NOTE: Increasing this would break existing checkpoints - only change if starting fresh)
  action_space: "continuous"          # continuous or discrete
  action_range: [-1.0, 1.0]          # Position size range
  max_episode_steps: 10000            # Maximum steps per episode (episodes will terminate at this limit)
                                      # Set to null/None to use full data length (may be very long)
  lookback_bars: 20                   # Number of historical bars to use in state (NOTE: Increasing this would break existing checkpoints - only change if starting fresh)
  
  # Reward function parameters
  reward:
    pnl_weight: 1.0                   # PnL contribution to reward
    transaction_cost: 0.0001          # Transaction cost per trade
    risk_penalty: 0.5                  # Risk penalty coefficient
    drawdown_penalty: 0.3             # Drawdown penalty

# Training Configuration
training:
  total_timesteps: 1000000            # Total training steps
  save_freq: 10000                    # Save checkpoint frequency
  eval_freq: 5000                     # Evaluation frequency
  device: "cuda"                      # cuda or cpu
  num_envs: 1                         # Number of parallel environments
  
  # Data
  train_split: 0.8                    # Train/test split
  validation_split: 0.1               # Validation split
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 50000                   # Steps to wait without improvement
    min_delta: 0.01                   # Minimum improvement

# Risk Management
risk_management:
  max_position_size: 1.0              # Maximum position size (normalized)
  max_drawdown: 0.20                  # Maximum drawdown (20%)
  stop_loss_atr_multiplier: 2.0       # Stop loss as multiple of ATR
  initial_capital: 100000.0          # Starting capital (for paper trading)
  commission: 2.0                     # Commission per contract per side

# Reasoning Engine (LLM Provider)
reasoning:
  enabled: true                       # Enable reasoning layer
  provider: "ollama"                  # Provider: "ollama", "deepseek_cloud", or "grok"
  model: "deepseek-r1:8b"            # Model name (provider-specific)
  api_key: null                       # API key (required for cloud providers, set via environment variable or settings)
  base_url: null                      # Base URL (optional, uses provider defaults if null)
  pre_trade_validation: true         # Validate before trading
  confidence_threshold: 0.7           # Minimum confidence to trade
  timeout: 2.0                        # Reasoning timeout (seconds)

# Agentic Swarm Configuration
agentic_swarm:
  enabled: true                       # Enable agentic swarm system
  provider: "ollama"                  # Uses existing LLM provider from reasoning config
  max_handoffs: 10                    # Maximum agent handoffs
  max_iterations: 15                  # Maximum total iterations
  execution_timeout: 20.0             # Total execution timeout (seconds)
  node_timeout: 5.0                   # Per-agent timeout (seconds)
  cache_ttl: 300                      # Cache TTL (seconds) - 5 minutes
  
  # Market Research Agent Configuration
  market_research:
    instruments: ["ES", "NQ", "RTY", "YM"]  # Instruments to analyze
    correlation_window: 20            # Bars for correlation calculation
    divergence_threshold: 0.1         # Divergence detection threshold
  
  # Sentiment Agent Configuration
  sentiment:
    sources: ["newsapi"]              # Free sentiment sources
    newsapi_key: null                  # Set via NEWSAPI_KEY env var
    sentiment_window: 3600            # Sentiment window (seconds) - 1 hour
  
  # Analyst Agent Configuration
  analyst:
    deep_reasoning: true              # Enable deep reasoning
    conflict_detection: true           # Detect conflicts between sources
  
  # Recommendation Agent Configuration
  recommendation:
    risk_integration: true            # Integrate with RiskManager
    position_sizing: true             # Calculate position sizes

# Decision Gate Configuration
decision_gate:
  rl_weight: 0.6                      # RL agent weight (60%)
  swarm_weight: 0.4                   # Swarm agent weight (40%)
  min_combined_confidence: 0.7        # Minimum confidence to execute trade
  conflict_reduction_factor: 0.5      # Reduce confidence when RL and Swarm disagree
  swarm_enabled: true                 # Enable swarm integration
  swarm_timeout: 20.0                  # Swarm execution timeout (seconds)
  fallback_to_rl_only: true           # Fallback to RL-only if swarm fails

# Manual Approval Configuration
manual_approval:
  enabled: false                       # Enable manual approval workflow (requires UI integration)
  auto_approve_timeout: 30.0          # Auto-approve after timeout (seconds, 0 = no timeout)
  require_approval_for:               # Require approval for these conditions
    - "high_risk_trades"               # High risk trades
    - "conflicting_signals"            # When RL and Swarm disagree
    - "large_position_sizes"           # Large position sizes (>0.8)

# Logging
logging:
  log_dir: "logs"                     # Log directory
  tensorboard: true                   # Enable TensorBoard
  verbose: 1                          # Verbosity level (0, 1, 2)

# Live Trading Configuration
live_trading:
  enabled: false                      # Enable live trading (USE WITH CAUTION)
  paper_trading: true                 # Paper trading mode (default)

# Bridge Configuration
bridge:
  host: "localhost"                  # NT8 bridge server host
  port: 8888                         # NT8 bridge server port


# Continuous Learning Configuration
continuous_learning:
  retrain_frequency: 1000            # Retrain every N new experiences
  min_experiences: 500                # Minimum experiences before retraining
  evaluation_episodes: 10             # Episodes for model evaluation
  min_annotated_for_finetune: 100     # Minimum annotated experiences for DeepSeek fine-tuning
  experience_buffer_size: 10000       # Maximum experiences to store
  experience_storage: "data/experience_buffer"

