# Training Configuration for NT8 RL Trading Strategy

# Model Configuration
model:
  algorithm: "PPO"                    # PPO, SAC, DQN
  learning_rate: 0.0001               # Reduced for fine-tuning stability
  batch_size: 128                     # Batch size (increased for Turbo mode - with 50x multiplier = 6400 batch size)
  n_steps: 4096                       # Steps per update (INCREASED from 2048 - larger buffer = more data per update = more GPU work)
  n_epochs: 30                        # Epochs per update (increased from default 10 - more training per update = more GPU work)
  gamma: 0.99                         # Discount factor
  gae_lambda: 0.95                    # GAE lambda
  clip_range: 0.2                     # PPO clip range
  value_loss_coef: 0.5                # Value function loss coefficient
  entropy_coef: 0.025                 # Increased from 0.01 to encourage more exploration (prevents "no trading" convergence)
  max_grad_norm: 0.5                  # Gradient clipping
  
  # Network architecture (for future training - larger model = more VRAM usage)
  hidden_dims: [256, 256, 128]       # Larger model for better GPU utilization (was [128, 128, 64] in checkpoint)

# Environment Configuration
environment:
  instrument: "ES"                     # ES or MES
  timeframes: [1, 5, 15]             # Timeframes in minutes
  state_features: 905                 # State dimension (15 features * 3 timeframes * 20 lookback = 900 + 5 regime features = 905; must match environment)
  action_space: "continuous"          # continuous or discrete
  action_range: [-1.0, 1.0]          # Position size range
  max_episode_steps: 10000            # Maximum steps per episode (episodes will terminate at this limit)
                                      # Set to null/None to use full data length (may be very long)
  lookback_bars: 20                   # Number of historical bars to use in state (NOTE: Increasing this would break existing checkpoints - only change if starting fresh)
  
  trading_hours:
    enabled: true
    source_timezone: "America/New_York"   # NT8 export timezone (CME / US Eastern)
    exclude_weekends: true
    use_nt8_calendar: true
    sessions:
      - name: "Tokyo"
        timezone: "Asia/Tokyo"
        start: "09:00"
        end: "15:00"
      - name: "London"
        timezone: "Europe/London"
        start: "08:00"
        end: "16:30"
      - name: "New York"
        timezone: "America/New_York"
        start: "09:30"
        end: "16:00"
  
  # Reward function parameters (optimized for learning)
  reward:
    pnl_weight: 1.0                   # PnL contribution to reward
    transaction_cost: 0.0001          # Transaction cost per trade
    risk_penalty: 0.1                  # Further reduced to avoid excessive idle bleed
    drawdown_penalty: 0.1             # Reduced from 0.3 to allow positive rewards
    # CRITICAL FIX: Stop loss to cap losses (tightened from 2% to 1.5% to reduce average loss)
    stop_loss_pct: 0.015              # Stop loss at 1.5% of entry price (was 0.02 = 2%)
    min_risk_reward_ratio: 1.5        # Minimum risk/reward ratio (1.5:1) - reject trades with poor R:R
    # Phase 1: Regime-aware RL features
    include_regime_features: true      # Enable regime features in RL state (adds 5 features)

# Training Configuration
training:
  total_timesteps: 300000             # Fine-tuning steps (adjust as needed)
  save_freq: 10000                    # Save checkpoint frequency
  eval_freq: 5000                     # Evaluation frequency
  device: "cuda"                      # cuda or cpu
  num_envs: 1                         # Number of parallel environments
  transfer_learning: true             # Enable transfer learning
  transfer_checkpoint: "models/checkpoint_1950000.pt"  # Latest checkpoint (1.95M timesteps, state_dim=900)
  transfer_strategy: "copy_and_extend"  # Transfer learning strategy when architecture changes:
                                        # - "copy_and_extend": Copy weights, init new dims with small random values (recommended)
                                        # - "interpolate": Interpolate from existing neurons
                                        # - "zero_pad": Initialize new dimensions with zeros
  
  # Data
  train_split: 0.8                    # Train/test split
  validation_split: 0.1               # Validation split
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 20000                   # Earlier stop for fine-tune
    min_delta: 0.005                  # Minimum improvement

# Risk Management
risk_management:
  max_position_size: 1.0              # Maximum position size (normalized)
  max_drawdown: 0.20                  # Maximum drawdown (20%)
  stop_loss_atr_multiplier: 2.0       # Stop loss as multiple of ATR
  initial_capital: 100000.0          # Starting capital (for paper trading)
  commission: 2.0                     # Commission per contract per side
  max_position_fraction_of_balance: 0.02  # Max risk per trade relative to equity (2%)
  position_value_per_unit: 5000.0     # Notional value represented by position size 1.0
  break_even:
    enabled: true
    activation_pct: 0.0015            # Move to break-even after 0.15% favorable move
    trail_pct: 0.001                  # Trail stop 0.10% behind extremes once free-trade
    scale_out_fraction: 0.3           # Reduce by 70% when confluences drop
    scale_out_min_confluence: 2       # Trigger scale-out when confluences <= 2
    free_trade_fraction: 0.3          # Maintain 30% size once trade is free

# Drift Detection for Live Trading
drift_detection:
  enabled: true                       # Enable drift monitoring
  baseline_metrics:
    win_rate: 0.55                    # Baseline win rate
    sharpe_ratio: 1.0                 # Baseline Sharpe ratio
    profit_factor: 1.5                # Baseline profit factor
    max_drawdown: 0.15                # Baseline max drawdown
  thresholds:
    win_rate_drop: 0.10               # Alert if win rate drops by 10%
    sharpe_drop: 0.30                 # Alert if Sharpe drops by 0.3
    profit_factor_drop: 0.30          # Alert if PF drops by 30%
    max_drawdown_increase: 0.05       # Alert if DD increases by 5%
    consecutive_losses: 5             # Alert after 5 losses in a row
  window_size: 50                     # Number of trades to analyze
  min_trades: 20                      # Min trades before drift detection

# Reasoning Engine (LLM Provider)
reasoning:
  enabled: true                       # Enable reasoning layer
  provider: "ollama"                  # Provider: "ollama", "deepseek_cloud", or "grok"
  model: "deepseek-r1:8b"            # Model name (provider-specific)
  api_key: null                       # API key (required for cloud providers, set via environment variable or settings)
  base_url: null                      # Base URL (optional, uses provider defaults if null)
  keep_alive: "10m"                   # Keep model loaded in memory (10 minutes) for faster responses
  pre_trade_validation: true         # Validate before trading
  confidence_threshold: 0.7           # Minimum confidence to trade
  timeout: 2.0                        # Reasoning timeout (seconds)
  
  # Kong Gateway Integration (optional)
  use_kong: false                     # Route requests through Kong Gateway
  kong_base_url: "http://localhost:8300"  # Kong Gateway proxy URL
  kong_api_key: null                  # Kong consumer API key (set via KONG_API_KEY or provider-specific env var)

# Agentic Swarm Configuration
agentic_swarm:
  enabled: true                       # Enable agentic swarm system
  provider: "ollama"                  # Uses existing LLM provider from reasoning config
  keep_alive: "10m"                   # Keep model loaded in memory (10 minutes) for faster responses
  max_handoffs: 10                    # Maximum agent handoffs
  max_iterations: 15                  # Maximum total iterations
  execution_timeout: 20.0             # Total execution timeout (seconds)
  node_timeout: 5.0                   # Per-agent timeout (seconds)
  cache_ttl: 300                      # Cache TTL (seconds) - 5 minutes
  
  # Contrarian Agent Configuration (Warren Buffett-style overrides)
  contrarian:
    enabled: true                     # Enable contrarian agent by default
    greed_threshold_percentile: 90    # Top 10% sentiment/volatility = greed
    fear_threshold_percentile: 10     # Bottom 10% sentiment/volatility = fear

  # Market Research Agent Configuration
  market_research:
    instruments: ["ES", "NQ", "RTY", "YM"]  # Instruments to analyze
    correlation_window: 20            # Bars for correlation calculation
    divergence_threshold: 0.1         # Divergence detection threshold
  
  # Sentiment Agent Configuration
  sentiment:
    sources: ["newsapi"]              # Free sentiment sources
    newsapi_key: null                  # Set via NEWSAPI_KEY env var
    sentiment_window: 3600            # Sentiment window (seconds) - 1 hour
  
  elliott_wave:
    enabled: true
    lookback_bars: 400
    swing_threshold: 0.003
    min_confidence: 0.55
    position_multiplier: 0.6
    max_position_size: 0.8
    min_bars: 120
  
  # Analyst Agent Configuration
  analyst:
    deep_reasoning: true              # Enable deep reasoning
    conflict_detection: true           # Detect conflicts between sources
  
  # Recommendation Agent Configuration
  recommendation:
    risk_integration: true            # Integrate with RiskManager
    position_sizing: true             # Calculate position sizes

# Decision Gate Configuration
decision_gate:
  rl_weight: 0.6                      # RL agent weight (60%)
  swarm_weight: 0.4                   # Swarm agent weight (40%)
  min_combined_confidence: 0.7        # Minimum confidence to execute trade
  conflict_reduction_factor: 0.5      # Reduce confidence when RL and Swarm disagree
  swarm_enabled: true                 # Enable swarm integration
  swarm_timeout: 20.0                  # Swarm execution timeout (seconds)
  fallback_to_rl_only: true           # Fallback to RL-only if swarm fails
  position_sizing:
    enabled: true
    scale_multipliers:
      "1": 1.0
      "2": 1.15
      "3": 1.3
    max_scale: 1.3
    min_scale: 0.3
    hold_scale: 0.3
    rl_only_scale: 0.5
    swarm_only_scale: 0.65
    disagreement_scale: 0.35
    contrarian_threshold: 0.6
    elliott_threshold: 0.6

# Manual Approval Configuration
manual_approval:
  enabled: false                       # Enable manual approval workflow (requires UI integration)
  auto_approve_timeout: 30.0          # Auto-approve after timeout (seconds, 0 = no timeout)
  require_approval_for:               # Require approval for these conditions
    - "high_risk_trades"               # High risk trades
    - "conflicting_signals"            # When RL and Swarm disagree
    - "large_position_sizes"           # Large position sizes (>0.8)

# Logging
logging:
  log_dir: "logs"                     # Log directory
  tensorboard: true                   # Enable TensorBoard
  verbose: 1                          # Verbosity level (0, 1, 2)

# Live Trading Configuration
live_trading:
  enabled: false                      # Enable live trading (USE WITH CAUTION)
  paper_trading: true                 # Paper trading mode (default)

# Bridge Configuration
bridge:
  host: "localhost"                  # NT8 bridge server host
  port: 8888                         # NT8 bridge server port


# Continuous Learning Configuration
continuous_learning:
  retrain_frequency: 1000            # Retrain every N new experiences
  min_experiences: 500                # Minimum experiences before retraining
  evaluation_episodes: 10             # Episodes for model evaluation
  min_annotated_for_finetune: 100     # Minimum annotated experiences for DeepSeek fine-tuning
  experience_buffer_size: 10000       # Maximum experiences to store
  experience_storage: "data/experience_buffer"

